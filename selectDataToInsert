import pandas as pd
from elasticsearch import Elasticsearch
from elasticsearch import helpers

es_servers = [{
            "host": "192.168.20.72",
            "port": "9200"
        },
        {
            "host": "192.168.20.73",
            "port": "9200"
        },
        {
            "host": "192.168.20.74",
            "port": "9200"
        },
        {
            "host": "192.168.20.75",
            "port": "9200"
        },
        {
            "host": "192.168.20.76",
            "port": "9200"
        }]

# es_servers = [{
#             "host": "10.8.1.8",
#             "port": "9200"
#         }]

def gen(data):
    """ 使用生成器批量写入数据 """
    action = ({
        "_index": "judgement_result",
        "_type": "_doc",
        # "_id": str(data[i][11])+"_"+str(data[i][1]),
        "_id": str(data[i][11]),
        "_source": {
            "title": str(data[i][0]),
            # "key_word": str(data[i][1]),
            "content": str(data[i][2]),
            "source": str(data[i][3]),
            "url": str(data[i][4]),
            # "publish_time": str(data[i][5]),
            "publish_time": data[i][5],
            "create_time": str(data[i][6]),
            "raw_content": str(data[i][7]),
            "case_id": str(data[i][8]),
            "case_reason": str(data[i][9]),
            "court_name": str(data[i][10]),
            "docid": str(data[i][11]),
            "doctype": str(data[i][12]),
            "effect_hierarchy": str(data[i][13]),
            "judge_date": str(data[i][14]),
            "court_province": str(data[i][15]),
            "court_city": str(data[i][16]),
            "court_country": str(data[i][17]),
            "court_area": str(data[i][18]),
            "encase_type": str(data[i][19]),
            "update_time": str(data[i][20]),
            "source_type": str(data[i][21]),
            "abstract": str(data[i][22]),
            "picked_abstract": str(data[i][23]),
            "sentiment_type": str(data[i][24]),
            "sentiment_score": str(data[i][25]),
            # "accuser": str(",".join(data[i][26])),
            "accuser": str(data[i][26]),
            # "defendant": str(",".join(data[i][27])),
            "defendant": str(data[i][27]),
            "amount_of_fund": str(data[i][28]),
            "case_reason2": str(data[i][29]),
            "doctype2": str(data[i][30]),
            "district": str(data[i][31]),
            "level": str(data[i][31]),
            "province": str(data[i][33]),
            "city": str(data[i][34]),
            "other_info": str(data[i][35]),
            "company_name": str(data[i][36]),
            "company_name_title": str(data[i][37]),
            "other_name": str(data[i][38]),
            "case_type": str(data[i][39]),
            "judge_process": str(data[i][40])
        }
    } for i in range(len(data)))
    helpers.bulk(es, action)

def get_page_data(result):
    data = []
    for hit in result:
        # print(hit["_id"])
        a = hit["_id"].split("_")
        if a:
            docid = a[0]
            print(docid)
            # if docid == "47277c18ef86479a8b7cad830167b3d2":
            #     print(hit["_id"])
            if pd.isnull(docid) or docid == "":
                docid = 'qjd202108'
        source = hit["_source"]
        print(source["accuser"])
        publish_time = source["publish_time"]
        # print(publish_time)
        # print(type(publish_time))
        if ':' not in publish_time:
            publish_time = publish_time + ' 00:00:00'
        elif "T" in publish_time:
            publish_time = "0001-01-01 01:01:01"

        judge_date = source["judge_date"]
        if ':' not in judge_date:
            judge_date = judge_date + ' 00:00:00'
        elif "T" in judge_date:
            judge_date = "0001-01-01 01:01:01"
        data.append((source["title"],
            source["key_word"],
            source["content"],
            source["source"],
            source["url"],
            # source["publish_time"],
            publish_time,
            source["create_time"],
            source["raw_content"],
            source["case_id"],
            source["case_reason"],
            source["court_name"],
            # source["docid"],
            docid,
            source["doctype"],
            source["effect_hierarchy"],
            # source["judge_date"],
            judge_date,
            source["court_province"],
            source["court_city"],
            source["court_country"],
            source["court_area"],
            source["encase_type"],
            source["update_time"],
            source["source_type"],
            source["abstract"],
            source["picked_abstract"],
            source["sentiment_type"],
            source["sentiment_score"],
            source["accuser"],
            source["defendant"],
            source["amount_of_fund"],
            source["case_reason2"],
            source["doctype2"],
            source["district"],
            source["level"],
            source["province"],
            source["city"],
            source["other_info"],
            source["company_name"],
            source["company_name_title"],
            source["other_name"],
            source["case_type"],
            source["judge_process"]
                     ))
        print("数据长度：",len(data))
    # print(data)
    # try:
    gen(data)
    # print(data)
    # except:
    #     pass


page_line = 1000
es = Elasticsearch(es_servers, timeout=1000, max_retries=3, retry_on_timeout=True)
index="judgement_doc_analysis_result"
count = es.count(index=index)["count"]
print(count)
if (count % page_line == 0):
    page = (int) (count/page_line)
else:
    page = (int)(count/page_line + 1)

body={
    "query": {
        "bool":{
            "must":[
            {
                "match": {
                    "url":"4f7508dccc9e41aaa74dad2200ae89d1"
                }
            }
            # {
            #     "match": {
            #         "level":"基层"
            #     }
            # },
            # {
            #     "match": {
            #         "docid":"tb456"
            #     }
            # }
            ]

        }

    }
}

# body = {
#     "query":{
#         "match":{
#             "url":"http://wenshu.court.gov.cn/website/wenshu/181107ANFZ0BXSK4/index.html?docId=47277c18ef86479a8b7cad830167b3d2"
#         }
#     }
# }


rs = es.search(index=index, doc_type="_doc", body=body)
print(rs)
print(rs['hits']['total'])
get_page_data(rs["hits"]["hits"])

# def get_search_result(es, index, scroll='15m', timeout='10m', size=1000):
#     body = {"query": {
#         "match_all": {}
#     },
#         "from": 1,
#     }
#
#     queryData = es.search(
#         index=index,
#         doc_type="_doc",
#         scroll=scroll,
#         timeout=timeout,
#         size=size,
#         body=body
#     )
#
#
#     # mdata = queryData.get("hits").get("hits")
#
#     # if not mdata:
#     #     print('empty')
#     # else:
#     #     get_page_data(mdata)
#     # print('n:   ', n)
#     scroll_id = queryData["_scroll_id"]
#     # print("scroll_id:   ",len(scroll_id))
#     # total = queryData["hits"]["total"]["value"]
#     # print("total:   ", total)
#     for i in range(int(count / 1000)):
#         if i < 250:
#             continue
#
#         body = {"query": {
#             "match_all": {}
#         },
#             "from": i * 1000,
#         }
#
#         queryData = es.search(
#             index=index,
#             doc_type="_doc",
#             scroll=scroll,
#             timeout=timeout,
#             size=size,
#             body=body
#         )
#
#         print("i:", i)
#         res = es.scroll(scroll_id=scroll_id, scroll='15m')
#         # print('res: ',len(res))
#         get_page_data(res["hits"]["hits"])
#         # mdata = mdata + res["hits"]["hits"]
#         # print('last:    ',len(mdata))
#         # n = n + len(res)
#         # print(n)
#     # return mdata
#     return ""
#
# result = get_search_result(es, index)
# get_page_data(result)

# items = allDoc["hits"]["hits"]
# for i in items:
#     print(i["_source"])

# res = es.indices.delete("judgement_analysis_result")  # 删除索引
